<!DOCTYPE html>
<html>

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XB3PR2Y1TQ"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-XB3PR2Y1TQ');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Mind3D</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/css/bootstrap.min.css">
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/assets/css/Highlight-Clean.css">
    <link rel="stylesheet" href="/assets/css/styles.css">

    <link rel="apple-touch-icon" sizes="180x180" href="/assets/icon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/icon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/icon/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">

    <meta property="og:site_name" content="DreamFusion: Text-to-3D using 2D Diffusion" />
    <meta property="og:type" content="video.other" />
    <meta property="og:title" content="DreamFusion: Text-to-3D using 2D Diffusion" />
    <meta property="og:description" content="DreamFusion: Text-to-3D using 2D Diffusion, 2022." />
    <meta property="og:url" content="https://dreamfusion3d.github.io/" />
    <meta property="og:image" content="https://dreamfusion3d.github.io/assets/images/dreamfusion_samples.png" />

    <meta property="article:publisher" content="https://dreamfusion3d.github.io/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="DreamFusion: Text-to-3D using 2D Diffusion" />
    <meta name="twitter:description" content="We combine neural rendering with a multi-modal text-to-2D image diffusion generative model to synthesize diverse 3D objects from text." />
    <meta name="twitter:url" content="https://dreamfusion3d.github.io/" />
    <meta name="twitter:image" content="https://dreamfusion3d.github.io/assets/images/dreamfusion_samples.png" />
    <!-- <meta name="twitter:site" content="" /> -->

    <script src="assets/js/video_comparison.js"></script>
    <script type="module" src="https://unpkg.com/@google/model-viewer@2.0.1/dist/model-viewer.min.js"></script>
</head>

<body>

    <div class="highlight-clean" style="padding-bottom: 10px;">
        <div class="container" style="max-width: 768px;">
            <h1 class="text-center">Decoding the 3D Vision: 3D Object Reconstructions
            from fMRI Encodings of 2D Visual Stimuli</h1>
        </div>
        <div class="container" style="max-width: 768px;">
            <div class="row authors">
                <div class="col-sm">
                    <h5 class="text-center">Anonymous Author</h5>
                    <h6 class="text-center">Affiliation</h6>
                </div>
            </div>    
        </div>

    </div>
    <hr class="divider" />

    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Abstract</h2>
                <p>
                    When observing objects, the human brain inherently possesses high-level information related to 3D geometry and
                    semantics. However, there have been no significant efforts to extract 3D geometric structures from brain signals to understand and
                    quantify the mechanisms of 3D visual perception. Therefore, we introduced Mind3D, which uses UMAP projection to extract
                    advanced information from fMRI and uses a two-stage diffusion pipeline to gradually refine this information into 3D objects.
                    Mind3D achieves high-quality 3D reconstruction from fMRI, highlighting the presence of high-level geometric information
                    in human object perception. By analyzing the 3D generation of fMRI, we observed that the interaction between the left and
                    right hemispheres of the brain can improve visual quality. We also found a joint contribution between the visual brain region
                    and the medial temporal lobe (MTL) brain region in forming a comprehensive understanding of the observed object. V1 brain
                    regions excel in capturing features and silhouettes, while V2, V3, and V4 regions complement this by emphasizing texture and
                    color based on V1. These findings shed light on how the human brain processes high-level geometry alongside low-level
                    textures, providing valuable insights into the human visual perception mechanisms.
                </p>
            </div>
        </div>
    </div>

    <div class="container" style="max-width: 768px;">
        <div style="text-align: center;">
            <img src="assets/image/pipeline-1.png" style="width: 100%;">
        </div>
        <div style="text-align: center;">
            <img src="assets/image/pipeline1-1.png" style="width: 100%;">
        </div>
        <div style="text-align: center;">
            <img src="assets/image/pipeline2-1.png" style="width: 100%;">
        </div>
    </div>

    <hr class="divider" />
<div class="container" style="max-width: 768px;">
    <div class="row">
        <div class="col-md-12">
            <h2>Results</h2>

            <h4>Left and right hemispheres collaborate to improve the visual quality</h4>
                        <img src="assets/image/images1-1.png" style="width: 100%;">

            <p>
                <strong>a</strong> We experimented on the left and right hemispheres separately for feature extraction and 3D
                decoding.
                <strong>b</strong> There is no clear difference in the overall performance in the left and right hemispheres in 3D
                Inception, 3D
                CLIP, 3D EfficientNet, and 3D Swav. <strong>c-d</strong> The combination of both left and right hemispheres could
                reconstruct
                the object with better 3D Inception, 3D CLIP, 3D EfficientNet, and 3D Swav performance. <strong>e-f</strong> Left
                and right
                hemispheres show a close object feature perception ability in all evaluation metrics in every quantile. Boxen plot
                shows
                5&#37;, 15&#37;, 25&#37;, 50&#37;, 75&#37;, 85&#37;, and 95&#37; percentile. <strong>g-j</strong> Although the left
                and right hemispheres achieve a
                close overall reconstruction performance, their performance is quite different among each specific object.
                Coordinates
                of crosses (+) represent the performance from the left and right hemispheres in each reconstruction object.
                <strong>k</strong>
                Given similar example images, there is quite a difference in reconstruction objects from the left and right
                hemispheres.
                Their collaborated 3D scene exhibits an improvement in generation quality.
            </p>
        </div>
    </div>
</div>
    <hr class="divider" />

<div class="container" style="max-width: 768px;">
    <div class="row">
        <div class="col-md-12">
            <h4>Visual region predominates in comprehension</h4>
            <img src="assets/image/images2-1.png" style="width: 100%;">
            <p>
                <strong>a</strong> Visual regions exhibit better performance than MTL regions in 3D Inception, 3D CLIP, 3D
                EfficientNet and 3D
                Swav. <strong>b-c</strong> Vision region exhibits a higher proportion of objects with high 3D Inception and 3D CLIP
                scores.
                Crosses (+) represent the performance of each reconstruction object in each criterion. MTL vs all: 3D Inception:
                p=0.007,
                3D CLIP: p=0.05, 3D EfficientNet p=0.013 3D Swav p=0.017.* = p&lt;0.05, **=p&lt;0.01, ***=p&lt;0.001.
                <strong>d-g</strong> There is
                little positive correlation between vision and MTL region to perceive objects. Coordinates of crosses (+) represent
                the performance from vision and MTL in each reconstruction object. <strong>h</strong> MTL region tends to extract
                semantic
                features from the witnessed image, while the vision region concentrates on textures and silhouettes of the origin
                object.
            </p>

        </div>
    </div>
</div>


    <hr class="divider" />

<div class="container" style="max-width: 768px;">
    <div class="row">
        <div class="col-md-12">
            <h4>V1 Regions excel in features and silhouettes</h4>
                    <img src="assets/image/images3-1.png" style="width: 100%;">

<p>
    <strong>a</strong> V1 regions show higher 3D Inception, 3D CLIP, and lower 3D EfficientNet and 3D Swav score,
    achieving better performance among all visual regions in object perception.
    <strong>b-e</strong> V1 Regions exhibit better performance than V2, V3 and V4 to extract and reconstruct objects.
    <!-- <strong>b</strong> V1 vs. (V2, V3, V4) p=(0.11, 0.048,0.016), (V1, V2, V3, V4) vs. all p=(0.18,
    0.0015,0.00062,0.000069).
    <strong>c</strong> V1 vs. (V2, V3, V4) p=(0.010,0.025,0.050), (V1, V2, V3, V4) vs. all p=(0.93,0.0075, 0.019,
    0.039).
    <strong>d</strong> V1 vs. (V2, V3, V4) p=(0.00010, 0.0021,0.0037), (V1, V2, V3, V4) vs. all p=(0.99,0.000035,
    0.0011, p=0.002).
    <strong>e</strong> V1 vs. (V2, V3, V4) p=(0.088,0.272,0.095), (V1, V2, V3, V4) vs. all p=(0.21,0.0048, 0.024,
    0.0054).
    * = p&lt;0.05, ** = p&lt;0.01, *** = p&lt;0.001, **** = p&lt;0.0001. -->
    <strong>f</strong> V2 &amp; V3, V2 &amp; V4 shows a high positive correlation in the correlation scatter plot of 3D
    CLIP.
    <strong>g</strong> V1 region extracts and restores features and silhouettes of the given image, while V2, V3, and V4
    concentrate on a fraction of texture details.
</p>
        </div>
    </div>
</div>



    <hr class="divider" />
    
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h4>Low-Level and High-Level Embedding are both detrimental in visual processing</h4>
                <img src="assets/image/images4-1.png" style="width: 100%;">
    
<p>
    <strong>a-d</strong> Integration of both Low-Level and High-Level is beneficial for human brains to better perceive
    objects. Boxplots exhibit the median, the 25th, and 75th percentiles as box edges among the four metrics. Crosses
    (+) represent the scores of each image with the category displayed to participants.
    <strong>e</strong> High-level embedding in fMRI concentrates on abstract semantic information, while low-level
    embedding in fMRI focuses on color and texture information. Their combination achieves the perception of objects in
    human brains with both semantic and texture features.
</p>

            </div>
        </div>
    </div>







<hr class="divider" />

<div class="container" style="max-width: 768px;">
    <div class="row">
        <div class="col-md-12">
            <h4>Semantic stage and UMAP projection both improve reconstruction quality</h4>
            <img src="assets/image/images5-1.png" style="width: 100%;">
<p>
    <strong>a</strong> The semantic stage outperforms the perception stage in all evaluation metrics in every quantile.
    Boxen plot shows 5&#37;, 15&#37;, 25&#37;, 50&#37;, 75&#37;, 85&#37;, and 95&#37; percentile.
    <strong>b</strong> The semantic stage exhibits a larger proportion of high 3D Inception and 3D CLIP scores and a
    left shift of 3D EfficientNet and 3D Swav scores.
    <strong>c</strong> The semantic stage 3D objects show higher fidelity than the perception stage.
    <strong>d</strong> UMAP projection improves reconstruction quality among all metrics in every quantile.
    <strong>e</strong> Employing UMAP projection achieves more scenes with high 3D Inception and 3D cLIP scores and
    enables smaller 3D EfficientNet and 3D Swav scores.
    <strong>f</strong> UMAP projection improves textural stability in fMRI reconstruction.
</p>

        </div>
    </div>
</div>






    </div>
    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>3D scenes from fMRI</h2>
                <p>We also present videos of 3D objects reconstructed through our Mind3D by decoding fMRI of participants, 
                    which offers a thorough overview of 3D details. 
                    It may takes a few seconds for our website to load videos.</p>
            </div>
        </div>
    </div>
        <!-- <div class="container" style="max-width: 768px;">
            <div class="row">
                <div class="col-md-12">
                    <p>
                        3D scenes                      </p>
                </div>
            </div>
        </div> -->
<div class="container" style="max-width: 768px; display: flex; justify-content: space-between;">
    <div class="half" style="width: 25%; text-align: center;">
        <p>images presented</p>
    </div>
    <div class="half" style="width: 75%; text-align: center;">
        <p>3D objects reconstructed from fMRI of participants</p>
    </div>
</div>
<div class="container" style="max-width: 768px;">

<div class="row">


</div>
<div class="container" style="max-width: 768px;">

<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\39.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\39.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\9.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\9.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\222.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\222.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\290.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\290.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\145.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\145.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\147.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\147.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\177.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\177.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\340.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\340.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\77.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\77.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\369.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\369.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\360.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\360.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\79.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\79.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\110.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\110.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\112.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\112.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\325.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\325.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\42.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\42.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\46.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\46.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\55.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\55.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\90.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\90.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\289.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\289.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\41.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\41.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\22.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\22.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\11.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\11.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\44.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\44.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\85.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\85.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\209.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\209.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\323.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\323.mp4" type="video/mp4">
        </source>
    </video>
</div>
<div class="row captioned_videos" style="display: flex; align-items: center;">
    <img src="assets\video\images_gt\10.png" style="width: 25%;">
    <video class="video lazy" loop playsinline autoplay muted style="width: 75%;">
        <source src="assets\video\video_fine_new\10.mp4" type="video/mp4">
        </source>
    </video>
</div>


    <!-- 62,66 -->

<!-- 



    </div>
    <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>More ablation studies</h2>
                                <p>
                we present a series of additional ablation studies that extend beyond the scope of the main manuscript. These
                investigations delve into various facets of our model's performance and behavior, including: (1) the role of CLIP
                regularization in texture and shape refinement; (2) the
                benefits of the warm-up phase in the coarse stage;(3) the capabilities in generating human-centric 3D models;
                (4)the impact of using different initial images and random seeds
                for initialization; (5) the analysis of the effects with and with-
                out Alpha Entropy Regulation, and Smoothness; (6) the effects of utilizing Zero-1-to-3 during the refinement stage.
                                </p>
            </div>
        </div>
    </div>



    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>CLIP Regulation</h2>
                <p>

            During the coarse stage of our model, an additional CLIP loss is integrated to refine the generative process,
            significantly enhancing shapes and textures. The figure below exemplifies this improvement: the
            surfaces of the miniature schnauzer and piglet display considerably sharper textures subsequent to the application of
            the CLIP loss. Notably, the facial features of the piglet, such as the eyes and nose, achieve enhanced realism in the final
            row. This can be attributed to the CLIP regulation's role in optimizing the generated scenes to align closely with the
            textual prompts via CLIP similarity metrics.
            
            </p>
            </div>
        </div>
    </div>
    <div class="container" style="max-width: 768px;">
        <div style="text-align: center;">
            <img src="assets/images/clip.png" style="width: 100%;">
        </div>
    </div>

    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Zero-1-to-3 warmup in the coarse stage</h2>
                <p>
                In the beginning of the coarse stage,
                we implement the Zero-1-to-3 warmup with the guidance of RGB. During this phase, in the absence of DeepFloyd IF, the 3D
                object is able to achieve a stable, rough initialization, characterized by consistent geometric integrity. This
                foundational procedure benefits to reinforcing both the texture fidelity and geometric stability in the final 3D model
                outcomes. 
                Notably, post-warmup, the geometric configurations of objects, such as ice-cream sundaes and saguaro cactus,
                exhibit markedly improved realism. The implementation of the Zero-1-to-3 warmup lays the groundwork for a robust and
                realistic geometric framework in the nascent stages of model training.
                </p>
            </div>
        </div>
    </div>
    <div class="container" style="max-width: 768px;">
        <div style="text-align: center;">
            <img src="assets/images/warmup.png" style="width: 100%;">
        </div>
    </div>









        <div class="container" style="max-width: 768px;">
            <div class="row">
                <div class="col-md-12">
                <h2>Human-centric 3D Asset Generation</h2>
                <p>
                Our architecture demonstrates proficiency in the creation of human-related assets. By processing prompts that
                are specifically crafted for human figures, our architecture can generate 3D representations of human faces and
                bodies with high fidelity and consistency from multiple viewpoints. Remarkably, this is achieved without any
                predefined knowledge of human anatomical structures. For instance, our model successfully generates a realistic bust of
                an Asian Santa Claus. Additionally, our generated astronaut, Deadpool and Spiderman display a 3D geometric consistency that adheres closely to human anatomical form.
                    </p>
                </div>
            </div>
        </div>
    <div class="container" style="max-width: 768px;">
        <div style="text-align: center;">
            <img src="assets/images/human.png" style="width: 100%;">
        </div>
    </div>
    














        <div class="container" style="max-width: 768px;">
            <div class="row">
                <div class="col-md-12">
                <h2>Random Initialization</h2>
                <p>
                    We assess the impact of using different seeds and initialization images. Our findings indicate that our
                    method is not notably affected by the choice of random initialization. In contrast, DreamFusion and
                    Magic3D heavily relies on the selection of random seeds in achieving high-quality results and avoiding
                    the multi-face Janus problem.
                    </p>
                </div>
            </div>
        </div>
    <div class="container" style="max-width: 768px;">
        <div style="text-align: center;">
            <img src="assets/images/init.png" style="width: 100%;">
        </div>
    </div>
    
    
    </div>
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Smooth Regulation</h2>
                <p>
                    We conduct an evaluation of the impact of the smoothness loss on our coarse-stage results. Our findings
                    suggest that applying smoothness regulation can enhance the quality of textures. Simultaneously, it has
                    a modest beneficial effect on the quality of the mesh, as visualized via the surface normal
                    maps.
                </p>
            </div>
        </div>
    </div>
    <div class="container" style="max-width: 768px;">
        <div style="text-align: center;">
            <img src="assets/images/smooth.png" style="width: 100%;">
        </div>
    </div>
    
    
    </div>
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Entropy Loss</h2>
                <p>
                    We incorporate an additional entropy loss into our model, which aims to address floats and blurs in the
                    background that could enhance the mesh extraction process. We present a comparison of the results
                    obtained when training with and without the implementation of Entropy Loss. The use of entropy loss
                    effectively eliminates the floaters and blurs in the background, leading to clearer and more detailed
                    imagery.
                </p>
            </div>
        </div>
    </div>
    <div class="container" style="max-width: 768px;">
        <div style="text-align: center;">
            <img src="assets/images/entropy.png" style="width: 100%;">
        </div>
    </div>

    
    
    </div>
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Zero-1-to-3 in the Refinement Stage</h2>
                <p>
                We evaluate the effects of utilizing Zero-1-to-3 for surface normal regulation during the refinement stage. Similar to
                our approach in the coarse stage, we feed the surface normal rendering into Zero-1-to-3, while continuing to use
                Stable-Diffusion for RGB SDS loss. Utilizing only
                Stable-Diffusion to compute SDS losses for both surface normal and RGB texture renderings allows our model to generate
                more detailed mesh surfaces. This outcome is attributable to Zero-1-to-3's limitation in only generating low-resolution
                images and providing weak regulation at low resolutions, which renders it unsuitable for the refinement stage that
                necessitates more detailed textures and mesh surfaces.
                </p>
            </div>
        </div>
    </div>
    <div class="container" style="max-width: 768px;">
        <div style="text-align: center;">
            <img src="assets/images/zero123.png" style="width: 100%;">
        </div>
    </div>



    </div>
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>PBR Material Modeling</h2>
                <p>
                In addition to our previous experiments, we also investigate the use of the Physically-Based Rendering (PBR) material
                modeling method during our refinement stage. This method involves
                decomposing the texture into three components of the material model: the diffuse term, the roughness and metallic term,
                and the normal variation term. The PBR material modeling approach can be beneficial for simulation applications.
                </p>
            </div>
        </div>
    </div>
    <div class="container" style="max-width: 768px;">
        <div style="text-align: center;">
            <img src="assets/images/pbr.png" style="width: 100%;">
        </div>
    </div> -->


   <!-- <div class="container" style="max-width: 768px;">-->
<!--        <div class="row captioned_videos">-->
<!--            <div class="col-md-12">-->
<!--                &lt;!&ndash; Large format devices &ndash;&gt;-->
<!--                <video class="video lazy d-none d-xs-none d-sm-block" autoplay loop playsinline muted poster="https://dreamfusion-cdn.ajayj.com/sept28/wipe_opposite_6x4_smoothstep.jpg">-->
<!--                    <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/wipe_opposite_6x4_smoothstep.mp4" type="video/mp4"></source>-->
<!--                </video>-->
<!--                &lt;!&ndash; Small format devices &ndash;&gt;-->
<!--                <video class="video lazy d-xs-block d-sm-none" autoplay loop playsinline muted poster="https://dreamfusion-cdn.ajayj.com/sept28/shaded_3x3_smoothstep.jpg">-->
<!--                    <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/shaded_3x3_smoothstep.mp4" type="video/mp4"></source>-->
<!--                </video>-->
<!--                <h6 class="caption">Given a caption, DreamFusion generates relightable 3D objects with high-fidelity appearance, depth, and normals. Objects are represented as a Neural Radiance Field and leverage a pretrained text-to-image diffusion prior such as Imagen.</h6>-->
<!--            </div>-->
<!--        </div>-->
<!--    </div> -->
<!--    <hr class="divider" />-->
<!--    <div class="container" style="max-width: 768px;">-->
<!--        <div class="row">-->
<!--            <div class="col-md-12">-->
<!--                <h2>Generate 3D from text yourself!</h2>-->
<!--            </div>-->
<!--        </div>-->
<!--        <div class="row compositional captioned_videos">-->
<!--            <div class="col-sm-8 text">-->
<!--                <p class="selectable left" id="compositional_tags_depth_0"></p>-->
<!--            </div>-->
<!--            <div class="col-sm-4 my-auto">-->
<!--                <div class="video-compare-container">-->
<!--                    <video id="compositionalVideo" class="video" autoplay loop playsinline muted>-->
<!--                        <div class="screen" id="compositionalScreen"></div>-->
<!--                        <source id="compositionalVideoSrc" src="https://dreamfusion-cdn.ajayj.com/journey_sept28/cropped/full_continuous/a_DSLR_photo_of_a_squirrel___rgbdn_hq_15000.mp4" type="video/mp4">-->
<!--                    </video>-->
<!--                    &lt;!&ndash; <video onplay="resizeAndPlay(this)" style="height: 0px;" id="compositionalVideo" class="video lazy" autoplay loop playsinline muted>-->
<!--                        <source id="compositionalVideoSrc" data-src="https://dreamfusion-cdn.ajayj.com/journey_sept28/full/a_DSLR_photo_of_a_squirrel___rgbdn_hq_15000.mp4" type="video/mp4">-->
<!--                    </video> &ndash;&gt;-->
<!--                    &lt;!&ndash; <canvas height="752" class="videoMerge" id="compositionalVideoMerge" width="1002"></canvas> &ndash;&gt;-->
<!--                </div>-->
<!--                &lt;!&ndash; <h6 class="caption" id="compositionalCaption">a DSLR photo of a squirrel</h6> &ndash;&gt;-->
<!--            </div>-->
<!--        </div>-->
<!--    </div>-->
<!--    <hr class="divider" />-->
<!--    <div class="container" style="max-width: 768px;">-->
<!--        <div class="row">-->
<!--            <div class="col-sm-8">-->
<!--                <h2>Example generated objects</h2>-->
<!--                <p>DreamFusion generates objects and scenes from diverse captions. <a href="/gallery.html">Search through hundreds of generated assets in our full gallery.</a></p>-->
<!--            </div>-->
<!--            <div class="col-sm-4 my-auto">-->
<!--                <a href="/gallery.html" class="btn btn-primary btn-lg btn-search"><svg xmlns="http://www.w3.org/2000/svg" width="24px" height="24px" viewbox="0 0 600 550">-->
<!--                    <path fill="none" stroke="#fff" stroke-width="36" stroke-linecap="round" d="m280,278a153,153 0 1,0-2,2l170,170m-91-117 110,110-26,26-110-110"/>-->
<!--                    </svg>Search assets-->
<!--                </a>-->
<!--            </div>-->
<!--        </div>-->
<!--        <div class="row captioned_videos" id="randomVideos">-->
<!--            &lt;!&ndash; <div class="col-4">-->
<!--                <div class="video-compare-container" style="width: 100%">-->
<!--                    <video class="video lazy" id="ex1" loop playsinline autoplay muted onplay="resizeAndPlay(this)" style="height: 0px;">-->
<!--                        <source data-src="https://dreamfusion-cdn.ajayj.com/gallery/a_teddy_bear_pushing_a_shopping_cart_full_of_fruits_and_vegetables_rgbdn_hq_15000.mp4" type="video/mp4"></source>-->
<!--                    </video>-->
<!--                    <canvas height="752" class="videoMerge" id="ex1Merge" width="1002"></canvas>-->
<!--                </div>-->
<!--                <h6 class="caption">A teddy bear pushing a shopping cart full of fruits and vegetables.</h6>-->
<!--            </div>-->
<!--            <div class="col-4">-->
<!--                <div class="video-compare-container" style="width: 100%">-->
<!--                    <video class="video lazy" id="ex2" loop playsinline autoplay muted onplay="resizeAndPlay(this)" style="height: 0px;">-->
<!--                        <source data-src="https://dreamfusion-cdn.ajayj.com/gallery/a_sliced_loaf_of_fresh_bread_rgbdn_hq_15000.mp4" type="video/mp4"></source>-->
<!--                    </video>-->
<!--                    <canvas height="752" class="videoMerge" id="ex2Merge" width="1002"></canvas>-->
<!--                </div>-->
<!--                <h6 class="caption">a sliced loaf of fresh bread.</h6>-->
<!--            </div>-->
<!--            <div class="col-4">-->
<!--                <div class="video-compare-container" style="width: 100%">-->
<!--                    <video class="video lazy" id="ex3" loop playsinline autoplay muted onplay="resizeAndPlay(this)" style="height: 0px;">-->
<!--                        <source data-src="https://dreamfusion-cdn.ajayj.com/gallery/a_zoomed_out_DSLR_photo_of_Sydney_opera_house,_aerial_view_rgbdn_hq_15000.mp4" type="video/mp4"></source>-->
<!--                    </video>-->
<!--                    <canvas height="752" class="videoMerge" id="ex3Merge" width="1002"></canvas>-->
<!--                </div>-->
<!--                <h6 class="caption">a zoomed out DSLR photo of Sydney opera house, aerial view.</h6>-->
<!--            </div> &ndash;&gt;-->
<!--        </div>-->
<!--    </div>-->
<!--    <hr class="divider" />-->
<!--    <div class="container" style="max-width: 768px;">-->
<!--        <div class="row">-->
<!--            <div class="col-md-12">-->
<!--                <h2>Composing objects into a scene</h2>-->
<!--                &lt;!&ndash; <p>Our generated NeRF models can be exported to meshes using the marching cubes algorithm for easy integration into 3D renderers or modeling software.</p> &ndash;&gt;-->
<!--                <video class="video lazy" autoplay loop playsinline controls muted poster="https://dreamfusion-cdn.ajayj.com/carouselx24_128tall.jpg">-->
<!--                    <source src="https://dreamfusion-cdn.ajayj.com/carouselx24_128tall.mp4" type="video/mp4"></source>-->
<!--                </video>-->
<!--            </div>-->
<!--        </div>-->
<!--    </div>-->
   <!-- <hr class="divider" />
    <div class="container meshes" id="meshContainer" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Mesh exports</h2>
                <p>We can extract mesh from our fine stage result.</p>
            </div>
        </div>
    </div>
    <hr class="divider" /> -->
<!--    <div class="container" style="max-width: 768px;">-->
<!--        <div class="row">-->
<!--            <div class="col-md-12">-->
<!--                <h2>How does DreamFusion work?</h2>-->
<!--                <p>Given a caption, DreamFusion uses a text-to-image generative model called Imagen to optimize a 3D scene. We propose <strong>Score Distillation Sampling (SDS)</strong>, a way to generate samples from a diffusion model by optimizing a loss function. SDS allows us to optimize samples in an arbitrary parameter space, such as a 3D space, as long as we can map back to images differentiably. We use a 3D scene parameterization similar to Neural Radiance Fields, or NeRFs, to define this differentiable mapping. SDS alone produces reasonable scene appearance, but DreamFusion adds additional regularizers and optimization strategies to improve geometry. The resulting trained NeRFs are coherent, with high-quality normals, surface geometry and depth, and are relightable with a Lambertian shading model.</p>-->
<!--            </div>-->
<!--        </div>-->
<!--        <div class="row">-->
<!--            <div class="col-md-12">-->
<!--                <video class="video lazy" controls muted poster="https://dreamfusion-cdn.ajayj.com/dreamfusion_overview.jpg">-->
<!--                    <source data-src="https://dreamfusion-cdn.ajayj.com/dreamfusion_overview.mp4" type="video/mp4"></source>-->
<!--                </video>-->
<!--            </div>-->
<!--        </div>-->
<!--    </div>-->
<!--    <hr class="divider" />-->
<!--    <div class="container" style="max-width: 768px;">-->
<!--        <div class="row">-->
<!--            <div class="col-md-12">-->
<!--                <h2>Citation</h2>-->
<!--                <code>-->
<!--                    @article{poole2022dreamfusion,<br>-->
<!--                    &nbsp; author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},<br>-->
<!--                    &nbsp; title  = {DreamFusion: Text-to-3D using 2D Diffusion},<br>-->
<!--                    &nbsp; journal = {arXiv},<br>-->
<!--                    &nbsp; year   = {2022},<br>-->
<!--                }</code>-->
<!--            </div>-->
<!--        </div>-->
<!--    </div>-->
<!--    <script src="https://polyfill.io/v3/polyfill.js?features=IntersectionObserver"></script>-->
<!--    <script src="/assets/js/yall.js"></script>-->
<!--    <script>-->
<!--        yall(-->
<!--            {-->
<!--                observeChanges: true-->
<!--            }-->
<!--        );-->
<!--    </script>-->
    <!-- <script src="/assets/js/scripts.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"></script>
 -->

    <!-- <div class="container meshes" id="meshContainer2" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Image to 3D</h2>
                <p>Generate 3D meshes from the given poster image</p>
            </div>
        </div>
    </div>
    <hr class="divider" />
    <script src="/assets/js/scripts2.js"></script> -->
<!--    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>-->
<!--    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>-->
<!--    <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"></script>-->
    <!-- Import the component -->
    <!-- <div class="container" style="max-width: 768px;">
            <div class="text-center">Template from <a href="https://dreamfusion3d.github.io/">https://dreamfusion3d.github.io/</a></div>
    </div> -->
</body>

</html>
